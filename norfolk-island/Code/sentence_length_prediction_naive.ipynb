{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde49eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f07343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected dimension: 119\n",
      "X_train shape: (2705, 118)\n",
      "X_val shape: (902, 118)\n",
      "X_test shape: (902, 118)\n",
      "y_train shape: (2705,)\n",
      "y_val shape: (902,)\n",
      "y_test shape: (902,)\n"
     ]
    }
   ],
   "source": [
    "## Preprocessing\n",
    "\n",
    "# Enable and disable features\n",
    "\n",
    "data = pd.read_csv('VDL_Norfolk_Island_Penal_Colony_Cleaned_No_Outliers.csv')\n",
    "data = data.drop(columns=['trial_id'])\n",
    "\n",
    "TARGET_VARIABLE = 'pp_sentence_years'\n",
    "\n",
    "# A mapping to boolean indicating that whether a feature is enabled, do not use enabled_features\n",
    "# ENABLED_FEATURE_MAPPING = {\n",
    "#     'offence_pp_general': True,\n",
    "#     'offence_pp': True,\n",
    "#     'pp_sentence_years': True,\n",
    "#     'trial_month': True,\n",
    "#     'trial_year': True,\n",
    "#     'trial_place': True,\n",
    "#     'pris_ht_pp': True,\n",
    "#     'def_age_pp': True,\n",
    "#     'def_literacy': True,\n",
    "#     'def_religion_pp': True,\n",
    "#     'marital_status_pp': True,\n",
    "#     'children_nr': False,\n",
    "#     'cash_sav_pp': False,\n",
    "#     'occupation_pp': True,\n",
    "#     'coloffence_info': True,\n",
    "#     'offence_ni': True,\n",
    "#     'death_in_custody_pp': True,\n",
    "#     'length_of_stay_until_probat': False,\n",
    "#     'length_of_stay_until_tl': False,\n",
    "#     'previous_convictions': True\n",
    "# }\n",
    "# 4 disabled features\n",
    "\n",
    "\n",
    "# ENABLED_FEATURE_MAPPING = {\n",
    "#     'offence_pp_general': True,\n",
    "#     'offence_pp': False,\n",
    "#     'pp_sentence_years': True,\n",
    "#     'trial_month': False,\n",
    "#     'trial_year': False,\n",
    "#     'trial_place': False,\n",
    "#     'pris_ht_pp': False,\n",
    "#     'def_age_pp': True,\n",
    "#     'def_literacy': True,\n",
    "#     'def_religion_pp': True,\n",
    "#     'marital_status_pp': True,\n",
    "#     'children_nr': False,\n",
    "#     'cash_sav_pp': False,\n",
    "#     'occupation_pp': False,\n",
    "#     'coloffence_info': True,\n",
    "#     'offence_ni': True,\n",
    "#     'death_in_custody_pp': True,\n",
    "#     'length_of_stay_until_probat': False,\n",
    "#     'length_of_stay_until_tl': False,\n",
    "#     'previous_convictions': True\n",
    "# }\n",
    "# # offence_pp_general, def_age_pp, def_literacy, def_religion_pp, marital_status_pp, coloffence_info, offence_ni, death_in_custody_pp, previous_convictions\n",
    "\n",
    "\n",
    "ENABLED_FEATURE_MAPPING = {\n",
    "    'offence_pp_general': True,\n",
    "    'offence_pp': True,\n",
    "    'pp_sentence_years': True,\n",
    "    'trial_month': True,\n",
    "    'trial_year': True,\n",
    "    'trial_place': True,\n",
    "    'pris_ht_pp': True,\n",
    "    'def_age_pp': True,\n",
    "    'def_literacy': True,\n",
    "    'def_religion_pp': True,\n",
    "    'marital_status_pp': True,\n",
    "    'children_nr': True,\n",
    "    'cash_sav_pp': True,\n",
    "    'occupation_pp': True,\n",
    "    'coloffence_info': True,\n",
    "    'offence_ni': True,\n",
    "    'death_in_custody_pp': True,\n",
    "    'length_of_stay_until_probat': True,\n",
    "    'length_of_stay_until_tl': True,\n",
    "    'previous_convictions': True\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# A mapping to boolean indicating that whether a feature is numerical\n",
    "FEATURE_TYPE_MAPPING = {\n",
    "    'offence_pp_general': False,\n",
    "    'offence_pp': False,\n",
    "    'pp_sentence_years': True,\n",
    "    'trial_month': True,\n",
    "    'trial_year': True,\n",
    "    'trial_place': False,\n",
    "    'pris_ht_pp': True,\n",
    "    'def_age_pp': True,\n",
    "    'def_literacy': True,\n",
    "    'def_religion_pp': False,\n",
    "    'marital_status_pp': False,\n",
    "    'children_nr': True,\n",
    "    'cash_sav_pp': True,\n",
    "    'occupation_pp': False,\n",
    "    'coloffence_info': True,\n",
    "    'offence_ni': True,\n",
    "    'death_in_custody_pp': True,\n",
    "    'length_of_stay_until_probat': True,\n",
    "    'length_of_stay_until_tl': True,\n",
    "    'previous_convictions': True\n",
    "}\n",
    "\n",
    "# Select only the enabled features\n",
    "data = data[[col for col in data.columns if ENABLED_FEATURE_MAPPING[col] or col == TARGET_VARIABLE]]\n",
    "\n",
    "# Move the target variable to the end of the dataframe\n",
    "data = data[[col for col in data.columns if col != TARGET_VARIABLE] + [TARGET_VARIABLE]]\n",
    "\n",
    "# Move all numerical features to the left of the categorical features\n",
    "numerical_features = [col for col in data.columns if FEATURE_TYPE_MAPPING[col] and col != TARGET_VARIABLE]\n",
    "categorical_features = [col for col in data.columns if not FEATURE_TYPE_MAPPING[col] and col != TARGET_VARIABLE]\n",
    "data = data[numerical_features + categorical_features + [TARGET_VARIABLE]]\n",
    "\n",
    "# Normalize the numerical features\n",
    "def normalize_data(data):\n",
    "    for col in data.columns:\n",
    "        if FEATURE_TYPE_MAPPING[col] and col != TARGET_VARIABLE:\n",
    "            data[col] = (data[col] - data[col].mean()) / data[col].std()\n",
    "    return data\n",
    "data = normalize_data(data)\n",
    "\n",
    "# Expect total dimension num_numerical_features + num_categorical_features * number of unique values + 1\n",
    "dim_expected = len(numerical_features) + sum([len(data[col].unique()) for col in categorical_features]) + 1\n",
    "print(f\"Expected dimension: {dim_expected}\")\n",
    "\n",
    "# Backup data before one-hot encoding\n",
    "data_backup = data.copy()\n",
    "\n",
    "# # For each categorical feature, convert it to a one-hot encoding\n",
    "def one_hot_encode(data):\n",
    "    for col in data.columns:\n",
    "        if not FEATURE_TYPE_MAPPING[col] and col != TARGET_VARIABLE:\n",
    "            one_hot = pd.get_dummies(data[col], prefix=col)\n",
    "            data = data.drop(col, axis=1)\n",
    "            data = pd.concat([data, one_hot], axis=1)\n",
    "    return data\n",
    "data = one_hot_encode(data)\n",
    "\n",
    "assert data.shape[1] == dim_expected, f\"Data shape {data.shape[1]} does not match expected dimension {dim_expected}\"\n",
    "\n",
    "# Move the target variable to the end of the dataframe again\n",
    "data = data[[col for col in data.columns if col != TARGET_VARIABLE] + [TARGET_VARIABLE]]\n",
    "\n",
    "# Convert everything to float\n",
    "data = data.astype(float)\n",
    "\n",
    "# Convert to numpy array\n",
    "data = data.to_numpy()\n",
    "\n",
    "# Split the data into training, validation, and test sets 60/20/20\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f316b4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2013.07742874\n",
      "Iteration 2, loss = 1944.85201284\n",
      "Iteration 3, loss = 1843.66849028\n",
      "Iteration 4, loss = 1679.70640512\n",
      "Iteration 5, loss = 1432.81659880\n",
      "Iteration 6, loss = 1135.63022139\n",
      "Iteration 7, loss = 905.78470340\n",
      "Iteration 8, loss = 804.77447541\n",
      "Iteration 9, loss = 774.07542671\n",
      "Iteration 10, loss = 749.47948041\n",
      "Iteration 11, loss = 729.84086098\n",
      "Iteration 12, loss = 714.11093639\n",
      "Iteration 13, loss = 701.19573955\n",
      "Iteration 14, loss = 688.59374852\n",
      "Iteration 15, loss = 678.18512009\n",
      "Iteration 16, loss = 668.11627251\n",
      "Iteration 17, loss = 659.14931736\n",
      "Iteration 18, loss = 650.63519767\n",
      "Iteration 19, loss = 643.20840954\n",
      "Iteration 20, loss = 635.58699817\n",
      "Iteration 21, loss = 629.04146994\n",
      "Iteration 22, loss = 623.13133976\n",
      "Iteration 23, loss = 617.75534603\n",
      "Iteration 24, loss = 613.43237686\n",
      "Iteration 25, loss = 608.13835560\n",
      "Iteration 26, loss = 603.40296126\n",
      "Iteration 27, loss = 599.15413863\n",
      "Iteration 28, loss = 596.12504536\n",
      "Iteration 29, loss = 593.01872960\n",
      "Iteration 30, loss = 589.89999182\n",
      "Iteration 31, loss = 585.15525918\n",
      "Iteration 32, loss = 582.79070578\n",
      "Iteration 33, loss = 579.71278563\n",
      "Iteration 34, loss = 576.99785031\n",
      "Iteration 35, loss = 573.79486024\n",
      "Iteration 36, loss = 571.60449672\n",
      "Iteration 37, loss = 568.92648660\n",
      "Iteration 38, loss = 566.48514384\n",
      "Iteration 39, loss = 563.88260871\n",
      "Iteration 40, loss = 561.58326286\n",
      "Iteration 41, loss = 559.38991528\n",
      "Iteration 42, loss = 557.15406684\n",
      "Iteration 43, loss = 555.51843756\n",
      "Iteration 44, loss = 552.97672874\n",
      "Iteration 45, loss = 551.39642001\n",
      "Iteration 46, loss = 549.02240875\n",
      "Iteration 47, loss = 546.32903986\n",
      "Iteration 48, loss = 544.63425707\n",
      "Iteration 49, loss = 542.71802626\n",
      "Iteration 50, loss = 540.65947648\n",
      "Iteration 51, loss = 539.11355133\n",
      "Iteration 52, loss = 536.97448477\n",
      "Iteration 53, loss = 535.29070708\n",
      "Iteration 54, loss = 533.29846537\n",
      "Iteration 55, loss = 530.67017413\n",
      "Iteration 56, loss = 528.81748781\n",
      "Iteration 57, loss = 526.77420621\n",
      "Iteration 58, loss = 525.20432069\n",
      "Iteration 59, loss = 522.98254191\n",
      "Iteration 60, loss = 521.39562590\n",
      "Iteration 61, loss = 518.94335426\n",
      "Iteration 62, loss = 518.13299122\n",
      "Iteration 63, loss = 515.28067298\n",
      "Iteration 64, loss = 513.36541058\n",
      "Iteration 65, loss = 511.71380016\n",
      "Iteration 66, loss = 509.62870193\n",
      "Iteration 67, loss = 507.29574667\n",
      "Iteration 68, loss = 505.48408075\n",
      "Iteration 69, loss = 503.29745910\n",
      "Iteration 70, loss = 502.05336242\n",
      "Iteration 71, loss = 499.52827943\n",
      "Iteration 72, loss = 498.65017892\n",
      "Iteration 73, loss = 496.10682615\n",
      "Iteration 74, loss = 494.08560506\n",
      "Iteration 75, loss = 492.98453445\n",
      "Iteration 76, loss = 490.23853682\n",
      "Iteration 77, loss = 487.85864867\n",
      "Iteration 78, loss = 485.28216145\n",
      "Iteration 79, loss = 484.11158742\n",
      "Iteration 80, loss = 481.47589164\n",
      "Iteration 81, loss = 480.56535779\n",
      "Iteration 82, loss = 478.48247948\n",
      "Iteration 83, loss = 475.51329308\n",
      "Iteration 84, loss = 473.70643323\n",
      "Iteration 85, loss = 471.46692469\n",
      "Iteration 86, loss = 469.96572988\n",
      "Iteration 87, loss = 467.49929560\n",
      "Iteration 88, loss = 465.04948037\n",
      "Iteration 89, loss = 463.29623670\n",
      "Iteration 90, loss = 461.48586711\n",
      "Iteration 91, loss = 460.37867165\n",
      "Iteration 92, loss = 457.88383137\n",
      "Iteration 93, loss = 455.60477876\n",
      "Iteration 94, loss = 453.40679263\n",
      "Iteration 95, loss = 450.99649466\n",
      "Iteration 96, loss = 450.03474572\n",
      "Iteration 97, loss = 447.53646939\n",
      "Iteration 98, loss = 445.18775113\n",
      "Iteration 99, loss = 443.00954104\n",
      "Iteration 100, loss = 440.72722921\n",
      "Architecture: (75, 50), MSE: 1404.0056179416201, MAE: 29.446296671772668 , max_iter: 100, actual_iter: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dm/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "architectures = [\n",
    "    (75, 50), # 1 hidden layer × 50 neurons\n",
    "    # (100,), # 1 hidden layer × 100 neurons\n",
    "    # (100, 50) # 2 hidden layers × 100 and 50 neurons\n",
    "]\n",
    "\n",
    "scoring = 'neg_mean_squared_error'\n",
    "\n",
    "# Initialize the MLPRegressor\n",
    "score_list_mse = []\n",
    "score_list_mae = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "max_iter = 100\n",
    "for architecture in architectures:\n",
    "    model = MLPRegressor(\n",
    "        hidden_layer_sizes=architecture,\n",
    "        activation='relu',\n",
    "        solver='adam',\n",
    "        max_iter=max_iter,\n",
    "        random_state=42,\n",
    "        verbose=True,\n",
    "    )\n",
    "    # Compute the MSE score on the validation set\n",
    "    model.fit(X_train, y_train)\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    mse = np.mean((y_val - y_val_pred) ** 2)\n",
    "    mae = np.mean(np.abs(y_val - y_val_pred))\n",
    "\n",
    "    print(f\"Architecture: {architecture}, MSE: {mse}, MAE: {mae} , max_iter: {max_iter}, actual_iter: {model.n_iter_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0a645a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2013.07742874\n",
      "Iteration 2, loss = 1944.85201284\n",
      "Iteration 3, loss = 1843.66849028\n",
      "Iteration 4, loss = 1679.70640512\n",
      "Iteration 5, loss = 1432.81659880\n",
      "Iteration 6, loss = 1135.63022139\n",
      "Iteration 7, loss = 905.78470340\n",
      "Iteration 8, loss = 804.77447541\n",
      "Iteration 9, loss = 774.07542671\n",
      "Iteration 10, loss = 749.47948041\n",
      "Iteration 11, loss = 729.84086098\n",
      "Iteration 12, loss = 714.11093639\n",
      "Iteration 13, loss = 701.19573955\n",
      "Iteration 14, loss = 688.59374852\n",
      "Iteration 15, loss = 678.18512009\n",
      "Iteration 16, loss = 668.11627251\n",
      "Iteration 17, loss = 659.14931736\n",
      "Iteration 18, loss = 650.63519767\n",
      "Iteration 19, loss = 643.20840954\n",
      "Iteration 20, loss = 635.58699817\n",
      "Iteration 21, loss = 629.04146994\n",
      "Iteration 22, loss = 623.13133976\n",
      "Iteration 23, loss = 617.75534603\n",
      "Iteration 24, loss = 613.43237686\n",
      "Iteration 25, loss = 608.13835560\n",
      "Iteration 26, loss = 603.40296126\n",
      "Iteration 27, loss = 599.15413863\n",
      "Iteration 28, loss = 596.12504536\n",
      "Iteration 29, loss = 593.01872960\n",
      "Iteration 30, loss = 589.89999182\n",
      "Iteration 31, loss = 585.15525918\n",
      "Iteration 32, loss = 582.79070578\n",
      "Iteration 33, loss = 579.71278563\n",
      "Iteration 34, loss = 576.99785031\n",
      "Iteration 35, loss = 573.79486024\n",
      "Iteration 36, loss = 571.60449672\n",
      "Iteration 37, loss = 568.92648660\n",
      "Iteration 38, loss = 566.48514384\n",
      "Iteration 39, loss = 563.88260871\n",
      "Iteration 40, loss = 561.58326286\n",
      "Iteration 41, loss = 559.38991528\n",
      "Iteration 42, loss = 557.15406684\n",
      "Iteration 43, loss = 555.51843756\n",
      "Iteration 44, loss = 552.97672874\n",
      "Iteration 45, loss = 551.39642001\n",
      "Iteration 46, loss = 549.02240875\n",
      "Iteration 47, loss = 546.32903986\n",
      "Iteration 48, loss = 544.63425707\n",
      "Iteration 49, loss = 542.71802626\n",
      "Iteration 50, loss = 540.65947648\n",
      "Iteration 51, loss = 539.11355133\n",
      "Iteration 52, loss = 536.97448477\n",
      "Iteration 53, loss = 535.29070708\n",
      "Iteration 54, loss = 533.29846537\n",
      "Iteration 55, loss = 530.67017413\n",
      "Iteration 56, loss = 528.81748781\n",
      "Iteration 57, loss = 526.77420621\n",
      "Iteration 58, loss = 525.20432069\n",
      "Iteration 59, loss = 522.98254191\n",
      "Iteration 60, loss = 521.39562590\n",
      "Iteration 61, loss = 518.94335426\n",
      "Iteration 62, loss = 518.13299122\n",
      "Iteration 63, loss = 515.28067298\n",
      "Iteration 64, loss = 513.36541058\n",
      "Iteration 65, loss = 511.71380016\n",
      "Iteration 66, loss = 509.62870193\n",
      "Iteration 67, loss = 507.29574667\n",
      "Iteration 68, loss = 505.48408075\n",
      "Iteration 69, loss = 503.29745910\n",
      "Iteration 70, loss = 502.05336242\n",
      "Iteration 71, loss = 499.52827943\n",
      "Iteration 72, loss = 498.65017892\n",
      "Iteration 73, loss = 496.10682615\n",
      "Iteration 74, loss = 494.08560506\n",
      "Iteration 75, loss = 492.98453445\n",
      "Iteration 76, loss = 490.23853682\n",
      "Iteration 77, loss = 487.85864867\n",
      "Iteration 78, loss = 485.28216145\n",
      "Iteration 79, loss = 484.11158742\n",
      "Iteration 80, loss = 481.47589164\n",
      "Iteration 81, loss = 480.56535779\n",
      "Iteration 82, loss = 478.48247948\n",
      "Iteration 83, loss = 475.51329308\n",
      "Iteration 84, loss = 473.70643323\n",
      "Iteration 85, loss = 471.46692469\n",
      "Iteration 86, loss = 469.96572988\n",
      "Iteration 87, loss = 467.49929560\n",
      "Iteration 88, loss = 465.04948037\n",
      "Iteration 89, loss = 463.29623670\n",
      "Iteration 90, loss = 461.48586711\n",
      "Iteration 91, loss = 460.37867165\n",
      "Iteration 92, loss = 457.88383137\n",
      "Iteration 93, loss = 455.60477876\n",
      "Iteration 94, loss = 453.40679263\n",
      "Iteration 95, loss = 450.99649466\n",
      "Iteration 96, loss = 450.03474572\n",
      "Iteration 97, loss = 447.53646939\n",
      "Iteration 98, loss = 445.18775113\n",
      "Iteration 99, loss = 443.00954104\n",
      "Iteration 100, loss = 440.72722921\n",
      "Final evaluation on test set, Architecture: (75, 50), MSE: 1279.7627051022773, MAE: 28.04515672698975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dm/lib/python3.13/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation on test set\n",
    "best_architecture = architectures[0]\n",
    "model = MLPRegressor(\n",
    "    hidden_layer_sizes=best_architecture,\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=100,\n",
    "    random_state=42,\n",
    "    verbose=True,\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "mse = np.mean((y_test - y_test_pred) ** 2)\n",
    "mae = np.mean(np.abs(y_test - y_test_pred))\n",
    "print(f\"Final evaluation on test set, Architecture: {best_architecture}, MSE: {mse}, MAE: {mae}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
