{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b3cef74-77cc-4ceb-9634-14bdd0b6b7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 8192\n",
    "end = 10240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8059b5a-0f82-4582-9032-7c6cf7c7e77c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperpararmeters Used: {'max_hidden_neurons': 1, 'P': 0.5, 'D': 0, 'lrs': [0.1, 0.0025, 0.0005]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script finds the best hyperparameter combination using Grid Search\n",
    "\n",
    "While running, it also builds up a one-to-one mapping between all possible 2^12 hyperparameter combiniations \n",
    "(encoded as Chromosoomes) and saved in 'results/fitness_table.csv'\n",
    "\n",
    "This script may takes one to two days to complete, if LOAD_RESULT, load the results 'results/fitness_table.csv'\n",
    "directly from my last run.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from utilities import import_data\n",
    "from utilities import set_seed\n",
    "from casper_test import final_test\n",
    "from casper_test import rkf_validator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from hypers_grid_search import chromosome_to_hyperparameters\n",
    "\n",
    "# Load from the result of Grid Search instead of running it again\n",
    "# This script may takes one to two days to complete\n",
    "LOAD_RESULT = True\n",
    "\n",
    "# Choose the best hyperparameter combination from the hyperparameter searches and fix it\n",
    "TOP_10S = pd.read_csv('results/top_10s.csv', index_col=0, dtype={0: str})\n",
    "\n",
    "\n",
    "HYPERPARAMETERS = chromosome_to_hyperparameters(TOP_10S.index[0])\n",
    "print(f'Hyperpararmeters Used: {HYPERPARAMETERS}')\n",
    "\n",
    "SEARCH_SPACE_SIZE = 2 ** 15\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "    \n",
    "# make results determinstic\n",
    "SEED = 4660\n",
    "if SEED != None:\n",
    "    set_seed(SEED)\n",
    "\n",
    "DNA_SIZE =15\n",
    "\n",
    "# Number of training for each hyperparameter combination\n",
    "N_SPLITS = 10\n",
    "N_REPEATS = 4\n",
    "\n",
    "def chromosome_to_dataset(chromosome, dataset):\n",
    "    \"\"\"\n",
    "    Convert a chromosome into a dataset with a subset of the features\n",
    "    Returns a dataset with the subset of features based on the chromosome\n",
    "    \"\"\"\n",
    "    if isinstance(chromosome, str):\n",
    "        # Convert the binary string to a list of integers\n",
    "        feature_indices = [int(bit) for bit in chromosome]\n",
    "    elif isinstance(chromosome, (list, np.ndarray)):\n",
    "        feature_indices = chromosome\n",
    "    else:\n",
    "        raise ValueError(\"Chromosome should be either a 15 length binary string or a 15 length number binary array\")\n",
    "    \n",
    "    if len(feature_indices) != 15:\n",
    "        raise ValueError(\"Chromosome length should be 15\")\n",
    "    \n",
    "    # Get the columns to keep based on the chromosome\n",
    "    columns_to_keep = [dataset.columns[0]] + [dataset.columns[i+1] for i, bit in enumerate(feature_indices) if bit]\n",
    "    \n",
    "    return dataset[columns_to_keep]\n",
    "\n",
    "def int_to_15bit_binary(num):\n",
    "    return bin(num)[2:].zfill(15)\n",
    "\n",
    "def create_table():\n",
    "    chromosomes = []\n",
    "    MSEs = []\n",
    "\n",
    "    fitness_table = pd.DataFrame({\n",
    "    'Chromosome': chromosomes,\n",
    "    'MSE': MSEs\n",
    "    })\n",
    "    fitness_table.set_index('Chromosome', inplace=True)\n",
    "    return fitness_table\n",
    "\n",
    "def save_table(fitness_table):\n",
    "    fitness_table.to_csv(f'results/{start}-{end}.csv')\n",
    "\n",
    "def grid_search(train_data):\n",
    "    \"\"\"\n",
    "    Compute the MSE for models training on all 2^12 possible subsets of features and save it in 'results/feature_selection_fitness.csv'\n",
    "    \"\"\"\n",
    "    fitness_table = create_table()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Give infinite MSE to empty set\n",
    "    for i in range(start, end):\n",
    "        chromosome = int_to_15bit_binary(i)\n",
    "        train_data_subset = chromosome_to_dataset(chromosome, train_data)\n",
    "        MSE = rkf_validator(train_data_subset, HYPERPARAMETERS, N_SPLITS, N_REPEATS, device=DEVICE, fast_mode=True, verbose=False)\n",
    "        fitness_table.loc[chromosome] = MSE\n",
    "        print(f\"Searched Feature Subset: {i + 1}/{end}   MSE: {MSE}\", end = '\\r')\n",
    "        # Save the progross every 64 search\n",
    "        if (i + 1) % 32 == 0:\n",
    "            save_table(fitness_table)\n",
    "    print()\n",
    "    print(f\"Grid Search Finished, Spent: {time.time() - start_time}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def54462-94f5-47b2-a333-b31ed4f5a2fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searched Feature Subset: 8456/10240   MSE: 2.2787516176700593\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data, _, _ \u001b[38;5;241m=\u001b[39m import_data()\n\u001b[1;32m      2\u001b[0m train_data, test_data, _, _ \u001b[38;5;241m=\u001b[39m train_test_split(data, data\u001b[38;5;241m.\u001b[39miloc[:,\u001b[38;5;241m0\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mSEED)\n\u001b[0;32m----> 3\u001b[0m grid_search(train_data)\n",
      "Cell \u001b[0;32mIn[2], line 97\u001b[0m, in \u001b[0;36mgrid_search\u001b[0;34m(train_data)\u001b[0m\n\u001b[1;32m     95\u001b[0m chromosome \u001b[38;5;241m=\u001b[39m int_to_15bit_binary(i)\n\u001b[1;32m     96\u001b[0m train_data_subset \u001b[38;5;241m=\u001b[39m chromosome_to_dataset(chromosome, train_data)\n\u001b[0;32m---> 97\u001b[0m MSE \u001b[38;5;241m=\u001b[39m rkf_validator(train_data_subset, HYPERPARAMETERS, N_SPLITS, N_REPEATS, device\u001b[38;5;241m=\u001b[39mDEVICE, fast_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     98\u001b[0m fitness_table\u001b[38;5;241m.\u001b[39mloc[chromosome] \u001b[38;5;241m=\u001b[39m MSE\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearched Feature Subset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m   MSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMSE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/COMP4660/Assignment 2/casper_test.py:120\u001b[0m, in \u001b[0;36mrkf_validator\u001b[0;34m(data, hyperparameters, n_splits, n_repeats, device, fast_mode, verbose, oversampling)\u001b[0m\n\u001b[1;32m    117\u001b[0m casper_net \u001b[38;5;241m=\u001b[39m CasperNet(input_dim, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m train(casper_net, data_train, hyperparameters, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, oversampling\u001b[38;5;241m=\u001b[39moversampling)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# evaluate the model\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fast_mode:\n",
      "File \u001b[0;32m~/Desktop/COMP4660/Assignment 2/casper.py:256\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data, hyperparameters, task, verbose, oversampling)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# train the network\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     optimiser \u001b[38;5;241m=\u001b[39m update_optimiser(model)\n\u001b[0;32m--> 256\u001b[0m     train_network(model, train_data, optimiser, P, task, device, verbose)\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hidden_neurons \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_hidden_neurons:\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/COMP4660/Assignment 2/casper.py:187\u001b[0m, in \u001b[0;36mtrain_network\u001b[0;34m(model, train_data, optimiser, P, task, device, verbose)\u001b[0m\n\u001b[1;32m    185\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m--> 187\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    188\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    189\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[38;5;28minput\u001b[39m, target, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/functional.py:3295\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3292\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3294\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m-> 3295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data, _, _ = import_data()\n",
    "train_data, test_data, _, _ = train_test_split(data, data.iloc[:,0], test_size=0.2, random_state=SEED)\n",
    "grid_search(train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
